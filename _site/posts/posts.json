[
  {
    "path": "posts/2020-06-08-the-base-rate-fallacy/",
    "title": "The Base Rate Fallacy",
    "description": "Explaining why we need Bayes in our everyday lives.",
    "author": [
      {
        "name": "Louisa H. Smith",
        "url": {}
      }
    ],
    "date": "2020-06-08",
    "categories": [],
    "contents": "\n\n\n.math-left-align .mjx-chtml.MJXc-display {\n    text-align: left !important;\n    font-size: 90% !important\n}\n.MathJax_Display {\n  text-align: left !important;\n}\n\nA recent opinion piece in the New York Times introduced the idea of the “Base Rate Fallacy.” We can avoid this fallacy using a fundamental law of probability, Bayes’ theorem. It sounds fancy but we actually already use it to reason in our everyday lives. I’ll motivate it with an example that is analogous to the COVID-19 antibody testing example from the NYT piece.\nHeadaches and brain tumors\nFor many people with brain tumors, one of the first signs is a headache. If you have a brain tumor, the probability that you have a headache is quite high. But most of us get headaches, and of course we know that if you have a headache, the probability that you have a brain tumor is quite low, or else we’d all be driving straight to the hospital with every headache. Why don’t we? Because brain tumors are extremely rare.\nSimilarly, if you’ve had COVID, the probability that you’ll test positive on an antibody test is quite high. In the media, people will say that the accuracy of these tests is great. But what does it mean if you test positive on an antibody test? Unfortunately, it doesn’t necessarily mean you actually have had COVID. Why not? Because – at least so far – COVID is quite rare.\nOf course, this depends on the population we’re looking at. We can define “population” any number of ways. For example, if you go to the doctor with a headache, they’ll judge the likelihood that you have a brain tumor, and therefore need more testing, based on the “base rate” of brain tumors in your population. If you live in Chernobyl, it’s more likely that a headache means you have a tumor than if you live in New York, because people with radiation exposure have a higher baseline risk of brain tumors. However, your “population” does not have to be defined geographically. If you have a history of another type of cancer, it’s more likely that a headache means you have a tumor than if you don’t. That’s because people with a history of cancer have the same small risk of a primary brain tumor as everyone else, but also have the additional risk of a metastasized tumor. Their base rate of brain tumors is higher, so a headache is more meaningful.\nThis is why a positive antibody test carries more meaning in New York than in, say, Maine. If you test positive in New York, it’s more likely you actually had COVID than if you test positive in Maine. And the higher the COVID prevalence in your “population,” defined however you like, the higher the probability that you really had the disease. So if you had symptoms at some point over the past few months, a positive antibody test is more likely to mean that you were actually infected, compared to if you didn’t have symptoms. This is because the “base rate” of COVID is higher among the population of people with symptoms than people without. The same would be true of essential workers, people who have partners who previously tested positive, etc.\nWe call this the “positive predictive value” (PPV) of a test. Formally, this is the probability that you are positive given that you test positive. When the prevalence of the condition (brain tumor, COVID) in the population is low, all else being equal, the PPV is low.\n\nPPV\n= Probability of being positive given a positive test\n= Pr(are positive | test positive)\n= Pr(brain tumor | headache)\n\n\n(Pr means “probability,” and | means “conditional on,” so you can read that last line “the probability of brain tumor given headache.”)\n\nWhat about negative tests?\nIf you don’t have a headache, what’s the likelihood that you don’t have a brain tumor? Extremely high, of course. If you have a negative antibody test, it’s quite likely that you never had COVID. When the prevalence of something is low, a negative test tells you a lot more than a positive test.\nWe call this the “negative predictive value” (NPV) of a test. This is the probability that you are negative given that you test negative. In other words, the probability that you don’t have a brain tumor given you don’t have a headache, or the probability that you don’t have a history of COVID given that you have a negative antibody test.\n\nNPV\n= Probability of being negative given a negative test\n= Pr(are negative | test negative)\n= Pr(no brain tumor | no headache)\n\nSo how can we say these tests are accurate?\nFirst of all, “accuracy” is a pretty meaningless word. Since PPV and NPV depend on the prevalence of the condition in the population, we generally judge how “good” tests are based on their sensitivity and specificity. These, like PPV and NPV, are probabilities relating testing positive or negative and actually being positive or negative. However, unlike PPV and NPV, they don’t depend on population prevalence (at least in this very simplified case we’re examining).\nIn our example, sensitivity is the probability that you will have a headache, given that you have a brain tumor. We said at the outset that’s quite high, so in that one sense a headache is a pretty good brain tumor test. Similarly, if you have indeed had COVID, you’re quite likely to have positive antibody test. In other words, how good is the test at picking up (sensing) this condition?\n\nsensitivity\n= Probability of testing positive given being positive\n= Pr(test positive | are positive)\n= Pr(headache | brain tumor)\n\nSpecificity is the same idea with respect to testing and being negative. If you don’t have a brain tumor, what’s the probability you don’t have a headache. (Not super high – there are a lot of other reasons for a headache! A headache is not a good test in this sense.)\n\nspecificity\n= Probability of testing negative given being negative\n= Pr(test negative | are negative)\n= Pr(no headache | no brain tumor)\n\nSimilarly, If you don’t have a history of COVID, what’s the probability you test negative on the antibody test. Quite high! A test that is good at ruling out a condition is highly specific.\nSo the COVID antibody tests generally meet both of our criteria for being a “good” test (without putting numbers to these values, since they depend on the test and other factors – it’s more complicated than I’ve made it out to be!). However, because many, many more people are truly negative for COVID than positive (the prevalence is low), a positive test result does not necessarily mean you have actually been infected.\nSo what is Bayes’ theorem?\nBayes’ theorem tells us how these probabilities relate, given the prevalence of the condition.\nFor events A and B, Bayes’ theorem is\n\n\n\n\\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\times \\Pr(A)}{\\Pr(B)}\n\\]\n\n\nwhich we can also write\n\n\n\n\\[\n\\Pr(A \\mid B) = \\]\\[\n\\frac{\\Pr(B \\mid A)\\times \\Pr(A)}{\\Pr(B \\mid A)\\times \\Pr(A) + \\Pr(B \\mid not \\;A)\\times \\Pr(not\\; A)}\n\\]\n\n\nWhat does this mean in terms of our examples? Well if the events of interest are “being positive” and “testing positive,” we can write this\n\n\n\n\\[\n\\Pr(\\text{are positive} \\mid \\text{test positive}) = \\]\\[\n\\frac{\\Pr(\\text{test pos} \\mid \\text{are pos}) \\times \\Pr(\\text{are pos})}{\\Pr(\\text{test pos} | \\text{are pos})\\Pr(\\text{are pos}) + \\Pr(\\text{test pos} | \\text{are neg})\\Pr(\\text{are neg})}\n\\]\n\n\nThen, using the above definitions and the fact that probabilities must add up to 1, we have\n\n\n\n\\[\n\\text{PPV} = \\]\\[\n\\frac{\\text{sensitivity} \\times \\text{prev}}{\\text{sensitivity} \\times \\text{prev} + (1 - \\text{specificity}) \\times (1 - \\text{prev})}\n\\]\n\n\nNow assume that sensitivity is 1: everyone with a brain tumor has a headache, or the test picks up on all the true COVID infections. This isn’t the case, but makes things easier and doesn’t really matter for our purposes. Then we have that\n\n\n\n\\[\n\\text{PPV} = \\]\\[\n\\frac{\\text{prev}}{\\text{prev} + (1 - \\text{spec}) \\times (1 - \\text{prev})}\n\\]\n\n\nNow let’s say that specificity is 0.95. In other words, 95% of people who never have COVID test negative (the other 5% are false positives). If prevalence is 1% (Maine, maybe) vs. 20% (New York, maybe), we have PPVs of \\(\\frac{.01}{.01 + .05 \\times .99} = 0.17\\) and \\(\\frac{.20}{.20 + .05 \\times .80} = 0.83\\), respectively. That means a positive test in Maine means that you only have 17% chance of truly having had the disease, but a positive test in New York gives you an 83% chance!\nNotice also that if specificity is 1, no matter the sensitivity or the prevalence, the PPV is also 1! Since there are no false positives, any positive test is a definitive positive. Unfortunately, that’s not true of many types of tests.\nYou can use the tool below to play around the the values to see what a positive or negative test means in various situations:\n\n\nBut why isn’t the test correct?\nThere are a number of reasons why a test might not be right. If you’re truly negative, it may pick up similar antibodies for another virus, it may be contaminated in the lab, it may be misread by a lab tech – any of these and more could lead to a false positive. If you’re truly positive, being tested too soon after infection, as well as various lab errors, could also lead to a false negative.\nThere are testing strategies to maximize the predictive values of tests. Tests can be conducted only in high prevalence populations, whether that’s defined by symptoms, exposure, or geographically. Repeat testing after a positive test can also increase the PPV. Tests with different properties can be used simultaneously to reduce error.\nWe encounter these concepts in everyday life, even if we’re not directly thinking about Bayes’ theorem. In order to reduce false positives, screening mammograms are only given in older women, as they have a much higher base rate of breast cancer than younger women. After a positive mammogram, repeat imaging and then a biopsy are subsequent tests used to rule out false positives. As COVID antibody testing becomes more widely available and new tests are developed, we can apply these principles in laying out a testing strategy. Importantly (and unfortunately), a positive test result does not currently mean that we can give up on social distancing and other precautions.\n\nThe CDC’s current guidelines on antibody testing are available here.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-18T12:24:50-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-09-25-github-style-calendar-heatmap/",
    "title": "Github-style calendar heatmap",
    "description": "Can I replicate the Github contributions calendar?",
    "author": [
      {
        "name": "Louisa H. Smith",
        "url": {}
      }
    ],
    "date": "2018-09-25",
    "categories": [],
    "contents": "\nI’ve spent a lot of time at the doctor’s office and hospital over the past year. I’ve been exploring different ways of visualizing this experience, and now that I have most of the rest of the year’s appointments in my calendar, I decided to get them out of my calendar and into R for some fun.\nThe inspiration for this visualization is the heatmap on Github that shows you how often you create commits, issues, etc. I’ve only recently started to use Github on an everyday basis (well, you can see below that I actually started back in January before I got sick), so my heatmap isn’t that impressive. (However, I did create my first issue – on someone else’s repository – recently, which I was really proud of!)\n\nMy goal was to recreate that style of heatmap calendar as closely as possible, with the colors representing how many appointments I had in a single day. I keep my medical appointments in their own calendar, so it was easy to download the .ics file from Google Calendar or export it from iCal. Then I used this online tool to convert the file into a .csv file. One problem I did run into was how repeating events were recorded. I have radiation every day for 3.5 weeks, mostly at the same time, so I had just created a repeating event for those appointments. However, this only showed up in the .csv file as one unique event, so I had to manually edit that and a couple of other things. Luckily it wasn’t a huge problem.\nTo start off, once I read the file into R I used some code for a similar heatmap I remembered seeing on this blog post. I grouped the appointments by day, counted the number per day, and organized them into weeks. That was straightforward enough. Then I had to do a lot of editing of the ggplot() theme to get the heatmap the way I wanted.\nI didn’t attempt to measure every aspect of the Githup heatmap, but I did look into the source code of the website to find the exact colors used. Luckily the most appointments I had in a day was four, which matched the four colors of green used on Github. The hardest thing was moving the legend to the appropriate place and then adding the “Less” and “More” labels next to it. I experimented with some different methods, including trying to add a bunch of white levels to the legend to shift it over, but finally I settled on one that I was moderately happy with. I’ve added individual annotate elements to a figure several times before, but this was my first time working outside of the plotting window, which was only a little bit exciting – mostly just annoying.\nHere’s the finished product:\n\nOriginally I didn’t start off with the title, as I didn’t plan on recreating the box and links around the heatmap, but I decided to add it anyway. Since there’s no link to tell you how I counted appointments, I’ll just say that I counted anything that required me to go to a different office – two “appointments” within the same clinic, one after the other, don’t count. Or else I’d have a lot more dark green!\nHere’s the code. Like I said earlier, the backbone was source code I found here. I also needed some serious Stack Overflow help for the text annotations!\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\ncal <- read_csv(here::here(\"code\", \"appointments.csv\"))\nby_day <- cal %>%\n  mutate(\n    date_time = paste(`DTSTART-DATE`, `START-TIME`),\n    date_time = mdy_hm(date_time),\n    # get the date of the sunday prior to each appointment\n    created_day = floor_date(date_time, \"day\"),\n    wday = wday(date_time)\n  ) %>%\n  select(created_day, wday) %>%\n  na.omit() %>% # (I had a blank line in the csv file)\n  mutate(\n    wday = factor(wday, levels = 1:7, labels = c(\n      \"Sun\", \"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\"\n    )),\n    # assign weeks and months for counting and labeling purposes\n    week = floor_date(created_day, \"week\"),\n    week = as_date(week),\n    month = floor_date(created_day, \"month\"),\n    month = as_date(month)\n  ) %>%\n  group_by(week, wday, month) %>%\n  # month is unnecessary to group by \n  # but it's a good trick to keep that variable!\n  count() %>%\n  mutate(\n    n = factor(as.character(n), levels = c(\"1\", \"2\", \"3\", \"4\")),\n    # add a 0 level to the 1:4 counts and move it to the front\n    n = fct_expand(n, \"0\"),\n    n = fct_relevel(n, \"0\")\n  )\n# match the format of the axis labels\nmonths <- seq(min(by_day$month), max(by_day$month), by = \"month\")\nmonth_labels <- strftime(months, \"%b\")\n# create the text annotations\nless <- grid::textGrob(\"Less\", gp = grid::gpar(fontsize = 10, col = \"#767676\"))\nmore <- grid::textGrob(\"More\", gp = grid::gpar(fontsize = 10, col = \"#767676\"))\np <- ggplot(by_day) +\n  aes(week, fct_rev(wday), fill = n) +\n  geom_tile(width = 7, height = 1) +\n  # decided to make the \"spaces\" lines \n  # instead of actual spaces between grey tiles\n  geom_hline(yintercept = seq(.5, 7.5, 1), col = \"white\", size = .85) +\n  geom_vline(\n    xintercept = seq(\n      as_date(\"2018-01-01\"), as_date(\"2018-12-31\"), by = \"week\"\n    ) + 2.5,\n    col = \"white\", size = .85\n  ) +\n  # the expand = F argument tells it to use those exact limits, no extra\n  coord_fixed(\n    ratio = 7, \n    xlim = c(min(by_day$month) + 2.5, max(by_day$month) + 25), \n    expand = FALSE\n    ) +\n  labs(x = NULL, y = NULL) +\n  scale_x_date(\n    expand = c(0, 0), breaks = months, labels = month_labels, position = \"top\"\n    ) +\n  scale_y_discrete(labels = c(\"\", \"Fri\", \"\", \"Wed\", \"\", \"Mon\", \"\")) +\n  scale_fill_manual(\n    limits = levels(by_day$n),\n    values = c(\"#EBEDF0\", \"#C6E48B\", \"#7BC96F\", \"#239A3B\", \"#196127\"),\n    name = NULL\n  ) +\n  theme(\n    # ugh so much trial and error to get these numbers:\n    legend.position = c(.904, -.405),\n    legend.justification = \"bottom\",\n    panel.grid = element_blank(),\n    panel.background = element_rect(fill = \"#EBEDF0\"),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_text(hjust = 0.5, color = \"#767676\", size = 10),\n    axis.text.y = element_text(color = \"#767676\", size = 10),\n    plot.margin = margin(4, 5, 4, 4),\n    legend.key.size = unit(10, \"pt\"),\n    legend.text = element_blank(),\n    legend.spacing.x = unit(.05, \"cm\"),\n    plot.title = element_text(hjust = 0, vjust = 0)\n  ) +\n  ggtitle(paste0(nrow(cal) - 1, \" appointments in the last year\")) +\n  guides(fill = guide_legend(nrow = 1)) +\n  annotation_custom(less,\n    xmin = as_date(\"2018-10-25\"),\n    xmax = as_date(\"2018-10-25\"), ymin = -2, ymax = -1\n  ) +\n  annotation_custom(more,\n    xmin = as_date(\"2018-12-20\"),\n    xmax = as_date(\"2018-12-20\"), ymin = -2, ymax = -1\n  )\n# this is necessary to get the annotations outside the plotting area to print\ngt <- ggplot_gtable(ggplot_build(p))\ngt$layout$clip[gt$layout$name == \"panel\"] <- \"off\"\ngrid::grid.draw(gt)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-18T12:24:48-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-09-11-secrets-and-robots-accessing-password-protected-data-with-r/",
    "title": "Secrets and Robots: Accessing password-protected data with R",
    "description": "In which I try to download my own insurance claims data.",
    "author": [
      {
        "name": "Louisa H. Smith",
        "url": {}
      }
    ],
    "date": "2018-09-11",
    "categories": [],
    "contents": "\nThere’s so much data out there, and yet I have access to so little of it. Of course, the most interesting data of all is that about me. As often as I can, I like to collect data on myself, or analyze data that others have collected on me.\nOne skill that I’ve been wanting to practice is web-scraping: gathering data from the internet. While trying to keep track of my insurance claims, I grew frustrated that my insurance company didn’t have an easy-to-use system for downloading or even just viewing that data. So I figured that attempting to collect my medical claims data would be perfect practice.\nWhile my last post highlighted a very basic attempt to get some data off of Wikipedia, this challenge would be much harder because my insurance data is password protected. Also, it doesn’t come in easy tabular form. First I tried and failed at using similar methodology as in my last post, in particular with the rvest or httr packages. While browsing this StackOverflow question and its links, I came across a better solution than trying to submit a form to a web session and hope it was working invisibly: use the RSelenium package. This tutorial was my most helpful resource, but I also got some hints from this site.\nWhat RSelenium does is allow you to navigate a web browser without actually touching the browser. You can write code to enter text, extract text, click, scroll, and anything else you might need to do. Let’s look at how it works.\nFirst, the setup. Since I needed to enter my password as text into the code, I used a tool in RStudio to avoid having to save that text in my code. When I run the askForPassword() function, up pops a box I can type my password into, which is then stored as the secret object. I also needed a list() object to store by data, and the url for my insurance company.\n\n\n# devtools::install_github(\"ropensci/RSelenium\")\nlibrary(RSelenium)\nlibrary(tidyverse)\nsecret <- rstudioapi::askForPassword() # not telling you my password!\ndat <- list()\nlogin_url <- \"https://www.bluecrossma.com/wps/portal/login/\"\n\n\n\nThis next bit of code I basically took directly off the internet. It opens a Google Chrome session (the default browser, though you can change that) behind my RStudio session and navigates to the given site. If you look at the browser, it reports that “Chrome is being controlled by automated test software.” I feel like a robot! If a website asked me to verify that I am not, I’m not sure that I could!\n\n\nrD <- rsDriver()\nremDr <- rD[[\"client\"]]\nremDr$navigate(login_url)\nSys.sleep(5) # make sure it loads the site before trying the next action\n\n\n\nThe next thing I needed to do was identify the text box on the login screen where I would enter my username and password. Right-clicking in a Chrome window and choosing “Inspect” allows you to navigate the source code behind any given page. Once you’ve found the code that points to the object you want, there are a few different ways to identify it. Finding that code is made easy by clicking the arrow in the upper-left corner of the inspection window, which allows you to mouse over the page and identify the corresponding code.\nHere’s the box for the username I’ll need to fill:\n\nI could use the “id” from the code, the “css selector” that shows up when you mouse over the box (which may not be unique, as we’ll see below), or the “xpath”, which you can copy with a right-click. I tend to use xpaths, except when they fail. Here they worked, for both the username and the password boxes. The findElement() function points to whatever object you’ve identified, after which you can sendKeysToElement(). This can be either text, like my username, or a key command, like pressing “enter” (as if on my keyboard) once my password is in.\n\n\nusername <- remDr$findElement(\n  using = \"xpath\", '//*[@id=\"ns_7_VA6HBFH200P070IALCTDO71085_username\"]'\n)\nusername$sendKeysToElement(list(\"louisahsmith\"))\npassword <- remDr$findElement(\n  using = \"xpath\", '//*[@id=\"ns_7_VA6HBFH200P070IALCTDO71085_password\"]'\n)\npassword$sendKeysToElement(list(secret, key = \"enter\")) # enter to submit\nSys.sleep(5) # pause to let load\n\n\n\nI put the Sys.sleep() commands in there so that the new page I’m directed to has time to load before I do my next action. It’s also helpful below when I’m toggling elements on a page on and off, and just in general so as not to bombard a site with requests.\nHere I’m looking for the link to review my claims. Trying to access it with its xpath wasn’t consistenly working (it apparently changed every time the page was reloaded), but I could identify that each of the four options was the same type of CSS object, and I selected all of them. Then I just had to choose the one that matched the text I was looking for, and click it.\n\n\n\nwebElems <- remDr$findElements(using = \"css selector\", \"li.large-3\")\nresHeaders <- unlist(lapply(webElems, function(x) {\n  x$getElementText() # read the text from each of the headers\n}))\nwebElem <- webElems[[which(\n  resHeaders == \"Review My Claims\\nReview my paid and/or pending claims.\"\n)]] # choose the header that corersponds to what I want\nwebElem$clickElement()\nSys.sleep(2)\n\n\n\nThe next page is where the data is. If it were better formatted, I could have just copy and pasted and saved myself all this trouble, since I’m only looking for my own claims and it’s not a process I’ll have to repeat over a number of people. However, the claims are spread across a number of pages, with a maximum of 20 per page, and some of the important information is hidden until you click the + button next to a claim.\n\n\nFirst I had to select the “View year-to-date” option, instead of just the past 30 days.\n\n\nelem2 <- remDr$findElement(using = \"id\", \"claimPeriod\")\nelem2$sendKeysToElement(list(\"y\", key = \"enter\")) # \"y\" for \"Year-to-date\"\nupdateBut <- remDr$findElement(using = \"id\", \"filterClaimsBtn\")\nupdateBut$clickElement()\nSys.sleep(10)\n\n\n\nThen I wrote a function to open all of the windows with the + button on a given page. It identifies the cells in each row by CSS type, then chooses the + one, and then clicks it. Because it takes a second to open each window, I gave it some time to do so with some more Sys.sleep() calls. After opening each one, I choose the data I want from the “Total” row, as well as some data from the original row, and keep it.\n\n\ndata_page <- function(remDr) {\n  toggs <- remDr$findElements(using = \"css selector\", \"td.dojoxGridCell\")\n  Sys.sleep(1)\n  toggvals <- unlist(lapply(toggs, function(x) {\n    x$getElementText()\n  }))\n  Sys.sleep(3)\n  # this is the data I want from the original rows\n  date <- toggvals[seq(2, length(toggvals), by = 9)] \n  type <- toggvals[seq(6, length(toggvals), by = 9)]\n  # these are the cells I want to click to open\n  goodtoggs <- toggs[which(toggvals == \"+\")]\n  lapply(goodtoggs, function(X) {\n    X$clickElement()\n    Sys.sleep(2)\n  })\n  # this is the data I want from the last row of the interior table\n  totals <- remDr$findElements(using = \"css selector\", \"tr.total\")\n  total_vals <- unlist(lapply(totals, function(x) {\n    x$getElementText()\n  }))\n  Sys.sleep(1)\n  keep <- tibble(date, type, total_vals)\n}\n\n\n\nI happen to know that so far I have 5 pages of claims from the past year, so I repeated this 5 times, going to the next page after each set of 20 windows was opened and read. Since this will change as more claims are processed, in the future I should rewrite this code so that it will identify how many pages there are without my help.\n\n\nfor (i in 1:5) {\n  # open the windows and gather data\n  dat[[i]] <- data_page(remDr)\n  \n  # go to the next page\n  nextBut <- remDr$findElement(using = \"id\", \"next\")\n  nextBut$clickElement()\n  Sys.sleep(5)\n}\nclaims <- reduce(dat, bind_rows)\n\n\n\nFinally I put it all together.\n\n\ndplyr::glimpse(claims)\n\n\nRows: 147\nColumns: 3\n$ date       <chr> \"11/03/2018\", \"11/03/2018\", \"10/17/2018\", \"10/16/…\n$ type       <chr> \"Pharmacy\", \"Pharmacy\", \"Medical\", \"Medical\", \"Me…\n$ total_vals <chr> \"Total $14.30 $119.99 $0.00\", \"Total $0.00 $355.9…\n\nThere’s still some cleaning to do, and of course the analysis part, but I’ll save that for another post!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-18T12:24:46-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-08-29-tidy-tuesday-california-fires/",
    "title": "Tidy Tuesday: California Fires",
    "description": "My first (very poor) attempt at a Tidy Tuesday challenge.",
    "author": [
      {
        "name": "Louisa H. Smith",
        "url": {}
      }
    ],
    "date": "2018-08-29",
    "categories": [],
    "contents": "\nI found out #tidytuesday is a thing on Twitter and wanted to join! Since I had lots of free time this past week, I had lofty goals. I wanted to try out gganimate, I wanted to incorporate data from another source, and I wanted to learn and practice some new functions, like those from the purrr package. Given the data on California wildfires, my vision was an animated graph that highlighted the “big fires” in California history. It didn’t end up exactly like I’d envisioned, but nothing ever does!\n\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\nlibrary(gganimate)\nlibrary(ggrepel)\nlibrary(grid)\n\n\n\nI started off by reading in the data and doing a little organizing (turns out I didn’t need to do most of it).\n\n\n# read in the data\nfires <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-08-21/week21_calfire_frap.csv\")\ndamage <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-08-21/cal-fire-incidents.csv\")\nstructures <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-08-21/calfire_damage.csv\")\ncause_levs <- c( # from the data dictionary\n  \"Lightning\", \"Equipment Use\", \"Smoking\", \"Campfire\", \"Debris\",\n  \"Railroad\", \"Arson\", \"Playing with Fire\", \"Miscellaneous\", \"Vehicle\",\n  \"Power Line\", \"Firefighter Training\", \"Non-Firefighter Training\",\n  \"Unknown/Unidentified\", \"Structure\", \"Aircraft\", \"Escaped Prescribed Burn\",\n  \"Illegal Alien Campfire\"\n)\n# restrict to California data and merge all three datasets\ndat <- fires %>%\n  filter(state == \"CA\") %>%\n  mutate(\n    cause = factor(cause, labels = cause_levs),\n    #alarm_date = parse_date(alarm_date),\n    #cont_date = parse_date(cont_date),\n    year = year_,\n    Name = str_to_title(parse_character(fire_name)),\n    `Start date` = format(alarm_date, \"%B %Y\")\n  ) %>%\n  full_join(damage, by = c(\"year\" = \"YEAR\")) %>%\n  full_join(structures)\n\n\n\nI found a Wikipedia article with some numbers on the biggest and most deadly fires. Perfect for my outside data source! I used the Web Inspector in Safari to find the appropriate xpaths and found that I would just need to replace one number in the path to get each of the tables I wanted.\nI had to make some changes to the data manually after I found out that not all of the Wikipedia fire names matched up to the ones in the provided data. This gave me an opportunity to use code like filter(dat, str_detect(str_to_lower(comments), \"complex\"),Start date== \"August 1999\") to find the fire I was looking for!\n\n\n# function to get the data from a table from a Wikipedia article, given the \n# url and the table number\nwiki_table <- function(url, num) {\n  url %>%\n    read_html() %>%\n    html_nodes(\n      xpath = str_replace('//*[@id=\"mw-content-text\"]/div/table[num]', \"num\", num)\n    ) %>%\n    html_table(fill = TRUE) %>%\n    .[[1]]\n}\n# get the 1st, 3rd, and 4th tables and merge\nwiki <- map(c(\"1\", \"3\", \"4\"), \n            .f = wiki_table,\n            url = \"https://en.wikipedia.org/wiki/List_of_California_wildfires\"\n            ) %>%\n  reduce(full_join) %>%\n  select(-Notes) %>%\n  # remove duplicates (first two with wrong numbers of dead)\n  filter(\n    !(Name == \"Witch\" & Deaths == 6),\n    !(Name == \"Thomas\" & Deaths == 1),\n    !(Name == \"Tubbs\" & Structures == \"5643\"),\n    parse_date_time(`Start date`, orders = \"mY\") >\n      min(dat$alarm_date, na.rm = T),\n    parse_date_time(`Start date`, orders = \"mY\") <\n      max(dat$alarm_date, na.rm = T)\n  ) %>%\n  # data cleaning to merge approriately\n  mutate(\n    Name = fct_recode(Name,\n      \"Bear Wallow-Lime Complex\" = \"Klamath Theater Complex\",\n      \"Marble-Cone\" = \"Marble Cone\",\n      \"Mcnally\" = \"McNally\",\n      \"Happy Camp\" = \"Happy Camp Complex\",\n      \"Harris 2\" = \"Harris\",\n      \"Redwood Valley\" = \"Redwood Valley Complex\",\n      \"Bel Air Fire\" = \"Bel Air\",\n      \"Laguna Fire\" = \"Laguna\",\n      \"Larson\" = \"Stanislaus Complex\"\n    ),\n    `Start date` = case_when(\n      Name == \"Marble-Cone\" ~ \"August 1977\",\n      TRUE ~ `Start date`\n    ),\n    Name = as.character(Name)\n  ) %>%\n  full_join(dat)\n\n\n\nIn my first attempt with gganimate I think I crashed R because I was trying to use too many datapoints. So I ended up grouping the data by month and calculating the cumulative total number of acres burned and people killed by fire.\n\n\n# group by month and calculate totals\nmonth_dat <- wiki %>%\n  mutate(Deaths = parse_number(Deaths)) %>%\n  group_by(`Start date`) %>%\n  summarise(\n    acres = sum(gis_acres, na.rm = T),\n    deaths = sum(Deaths, na.rm = T),\n    alarm = min(alarm_date, na.rm = T)\n  ) %>%\n  arrange(alarm) %>%\n  mutate(\n    cum_acres = cumsum(acres),\n    cum_deaths = cumsum(deaths),\n  # I wanted to highlight the changes with the big fires so I kept\n  # the previous value as well\n    pre_acres = lag(cum_acres, 1),\n    pre_deaths = lag(cum_deaths, 1),\n    date = parse_date_time(`Start date`, orders = \"mY\")\n  ) %>%\n  # I will need a \"time\" variable for the animation\n  # (rowid will correspond to month)\n  rowid_to_column() %>%\n  # Since I'll be facetting, I had to split the data by the two variables\n  gather(key = type, value = cum_num, cum_acres:pre_deaths) %>%\n  mutate(pre = type %in% c(\"pre_acres\", \"pre_deaths\"))\n\n\n\nI wanted a dataset with an observation each for acres burned and deaths caused, and a column each for the previous month’s value and the current month’s value. After using gather() on the appropriate variables, I had an observation for each variable at each time point, and I couldn’t figure out how to use spread() or something else to get it the way I wanted. So I did it sort of crudely:\n\n\npre_dat <- filter(month_dat, pre) %>% \n  rename(pre_val = cum_num) %>% \n  select(-pre) %>%\n  mutate(type = fct_recode(type, `Acres burned` = \"pre_acres\", Deaths = \"pre_deaths\"))\npost_dat <- filter(month_dat, !pre) %>% \n  rename(post_val = cum_num) %>% \n  select(-pre) %>%\n  mutate(type = fct_recode(type, `Acres burned` = \"cum_acres\", Deaths = \"cum_deaths\"))\nfull_dat <- full_join(pre_dat, post_dat) %>%\n  full_join(wiki) %>%\n  mutate(lab = case_when(\n    Acres > 100000 & type == \"Acres burned\" ~ Name,\n    Deaths > 0 & type == \"Deaths\" ~ Name,\n    TRUE ~ NA_character_\n  ),\n  pre_val = ifelse(is.na(lab), NA, pre_val))\n\n\n\nOK, here’s where things get interesting. I wanted to label my plot with the names of the fires. But some years had a lot of fires (especially last year 😢), so the text overlapped a great deal. The package ggrepel generally deals with that problem very nicely. BUT it calculates the placement of the text given whatever else is currently in the plot window, and if the text is progressively showing up in an animation, it recalulates the locations and moves the text with every frame, which looks ridiculous! So I attempted to get the positions from an initial call of geom_text_repel(), and use them as fixed positions in my ultimate animation. I found some help here but it was a lot of trial and error since I was also using facet_grid(), which meant two sets of coordinates. I didn’t spend a lot of time otherwise trying to make a pretty graph (I could spent weeks on that) but made it a little fiery 🔥.\n\n\n# initial plot -- want to be the finished product\np_repel <- ggplot(data = full_dat) +\n  geom_line(aes(x = date, y = post_val), col = \"#f03b20\", size = 1) +\n  geom_linerange(aes(ymin = pre_val, ymax = post_val, \n                     x = date), col = \"#ffeda0\", size = 1) +\n  geom_text_repel(aes(label = lab, x = date, y = post_val),\n    size = 4, col = \"#feb24c\", box.padding = 0) +\n  theme_dark() + theme(panel.grid = element_blank()) +\n  labs(\n    title = \"Major fires in California history (since 1950)\",\n    y = \"Cumulative number since 1950\", \n    x = \"Year\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  facet_grid(type ~ ., scales = \"free\") \np_repel\ngrid.force() # my understanding is that this kind of \"fixes\" the plot\n\n\n\n\n\n\np_build <- ggplot_build(p_repel)\n# need the axis limits\nxrg_acres <- p_build$layout$panel_params[[1]]$x.range\nyrg_acres <- p_build$layout$panel_params[[1]]$y.range\nxrg_deaths <- p_build$layout$panel_params[[2]]$x.range\nyrg_deaths <- p_build$layout$panel_params[[2]]$y.range\n# function to get the x and y values of a label's position\n# given which panel it is on\nget_pos_df <- function(n, panel) {\n  grb <- grid.get(n)\n  tibble(\n    # this rescales the axis values from a 0-1 scale to the actual scale\n    x = xrg_acres[1] + diff(xrg_acres) *\n      convertX(grb$x, \"native\", valueOnly = TRUE),\n    # there are two different y-axes because of the facets\n    y = switch(panel,\n      acres = yrg_acres[1] + diff(yrg_acres) *\n        convertY(grb$y, \"native\", valueOnly = TRUE),\n      deaths = yrg_deaths[1] + diff(yrg_deaths) *\n        convertY(grb$y, \"native\", valueOnly = TRUE)\n    ),\n    lab = grb$label,\n    type = panel\n  )\n}\n# get the positions for each of the labels from each panel\npos_data <- childNames(grid.get(\".*\", grep = TRUE)) %>%\n  str_subset(\"panel\") %>%\n  map(~childNames(grid.get(.))) %>%\n  # included \"segments\" as I was hoping they were the ggrepel lines\n  # they are not but I had to use some new functions to do that so\n  # I'm keeping the code!\n  list(string = ., pattern = c(\"textrepel\", \"segment\")) %>%\n  cross() %>%\n  map(lift(str_subset)) %>%\n  .[c(1, 2)] %>% # segment is not what I wanted to just chosing the repels\n  map(~childNames(grid.get(.))) %>%\n  map2_dfr(c(\"acres\", \"deaths\"), ~map_dfr(.x, get_pos_df, panel = .y)) %>%\n  mutate(\n    x = as_datetime(x),\n    type = factor(type, labels = c(\"Acres burned\", \"Deaths\"))\n  )\n\n\n\nOh my goodness, that was by far the hardest piece of code to get to work. I do not understand at all how plotting is done in R (but I guess I did learn some things)! This last chunk was really good practice with the purrr package, though. My code used ot be littered with a billion nested lapply() statements (and a lot of unlist() because nothing was ever in the form I wanted). Trying to stay away from that now!\nI needed some way to make sure the axes in my final plot have the appropriate limits. I could not find an easy way to do this while also using facet_grid(). I ended up making some data with the limits as observations that I plotted using geom_blank(), but there has got to be an easier way to do this! Annoyingly, I couldn’t get any animations to work when I used more than one dataset in a single plot, so I hard to merge everything together.\n\n\n# simple dataset with extreme values\nblank_dat <- tibble(\n  a = as_datetime(c(xrg_acres, xrg_deaths),\n    origin = \"1970-01-01 00:00.00 UTC\"\n  ),\n  b = c(yrg_acres, yrg_deaths),\n  type = rep(c(\"Acres burned\", \"Deaths\"), each = 2),\n  rowid = 1\n)\n# put the fire dataset together with the text positions and the blank data\n# gganimate didn't like me having them in different datasets\nplot_dat <- full_join(full_dat, pos_data) %>%\n  full_join(blank_dat)\n# most of this is the same as earlier, just with geom_text() instead\np_fixed <- ggplot(data = plot_dat) +\n  geom_line(aes(x = date, y = post_val), col = \"#f03b20\", size = 1) +\n  geom_linerange(aes(\n    ymin = pre_val, ymax = post_val,\n    x = date\n  ), col = \"#ffeda0\", size = 1) +\n  geom_text(aes(label = lab, x = x, y = y),\n    size = 4, col = \"#feb24c\"\n  ) +\n  theme_dark() + theme(panel.grid = element_blank()) +\n  labs(\n    title = \"Major fires in California history (since 1950)\",\n    y = \"Cumulative number since 1950\",\n    x = \"Year\"\n  ) +\n  # include blank data here so that scales will be the same\n  # as the geom_repel plot\n  geom_blank(aes(x = a, y = b)) +\n  # but then need to make sure there's no extra space around\n  # the blank data with expand()\n  scale_y_continuous(labels = scales::comma, expand = c(0, 0)) +\n  scale_x_datetime(expand = c(0, 0)) +\n  facet_grid(type ~ ., scales = \"free\")\n\n\n\nSo, weirdly, the text positions I had extracted from the “repelled” plot and used here to not actually correspond to the positions of the text in the former. I dug through the plotting object quite a bit and couldn’t find anything that matched up. And I have no idea where these positions came from because they have been “repelled” a little, just not fully. Who knows. At this point I gave up, not even bothering to try to add line segments to connect the text with the graph like ggrepel does, because my text never went very far away! I’m thinking the problem may have something to do with how the positions will be recalculated when the viewing window is resized. This doesn’t seem to have always been the case in the package, so may have messed up the original solution I found online? Or maybe it has something to do with my facetting? Again, who knows. I’m dropping it for now, before it consumes my life.\n\n\n# here's where the animation happens\np_anim <- p_fixed +\n  transition_time(rowid) +\n  # this tells everything to stay there for the next frame\n  shadow_mark()\ngif <- animate(p_anim,\n  fps = 5,\n  renderer = gifski_renderer(loop = TRUE),\n  ref_frame = -1\n)\ngif\n# anim_save(\"fires.gif\", animation = gif)\n\n\n\n\nDone! That was quite an expedition into a lot of new worlds at once. I enjoyed it but glad I did it during a week when I had much more free time than usual! My final product looks not nearly as good as I had imagined, partly because of the geom_text_repel() fail, but I’m proud of all the work it took to produce, and I learned a ton.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-18T12:24:43-07:00",
    "input_file": {}
  }
]
